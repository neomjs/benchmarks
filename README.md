# A Modern Benchmark Suite for Frontend Frameworks & Libraries

This repository hosts a modern performance benchmark suite designed to test the limits of front-end JavaScript frameworks. The showcases focus on scenarios involving high-frequency updates, heavy background computations, and sustained UI interactivity under loadâ€”areas critical for complex, data-intensive applications.

## Philosophy and Goals

Our goal is to create a fair, transparent, and community-driven environment for comparing different architectural approaches to front-end development. We aim to explore questions like:

- How do different frameworks maintain UI fluidity (e.g., 60 FPS) while processing thousands of real-time updates?
- What are the performance trade-offs between main-thread, worker-based, and server-side rendering architectures in complex, interactive applications?

We are committed to using established best practices for each framework to ensure the comparisons are meaningful and accurate.

## Call for Contributions

This is a community project. We welcome and encourage contributions, especially for implementing the benchmark scenarios in frameworks like **React, Solid, Svelte, Vue, and others**. If you are an expert in a particular framework, your help in building a high-quality, optimized implementation is invaluable.

Please see the [.github/EPIC-Performance-Showcases.md](.github/EPIC-Performance-Showcases.md) file for the technical specifications of the benchmark scenarios.

## Benchmark Harness

The harness is built with [Playwright](https://playwright.dev/) and is responsible for:
- Launching the showcase applications in a consistent environment.
- Automating user interactions to trigger specific performance tests.
- Recording videos and performance traces for analysis.
- Extracting key performance metrics using the browser's `performance` API.
- Generating structured reports with the benchmark results.

### Getting Started

1.  **Install Dependencies:**
    ```bash
    npm install
    ```

2.  **Build the Benchmark Application:**
    The tests can run against both a local development server and an optimized production build. To create the production build, run:
    ```bash
    npm run build:benchmark-app
    ```

### Running the Benchmarks

The suite is configured to run tests and automatically generate a `BENCHMARK_RESULTS.md` file. There are two modes for running the tests, and it is critical to choose the correct one.

-   **Functional Testing (Parallel Execution):**
    This mode is for **functional testing only** (i.e., "do the tests pass?"). It runs tests in parallel to finish quickly.
    ```bash
    npm run benchmark
    ```
    **Warning:** Do not use the performance numbers generated by this mode. Due to the multi-worker architecture of the Neo.mjs application, running tests in parallel creates severe CPU core contention, which leads to highly inaccurate and misleading performance results.

-   **Accurate Benchmarking (Serial Execution):**
    This is the **only valid mode for generating performance data**. It runs tests serially (one by one), ensuring that each test has dedicated access to system resources without interference. This process is slower but is essential for producing accurate and reliable benchmark numbers.
    ```bash
    npm run benchmark:accurate
    ```

### Results

The raw output from Playwright (videos, traces) is generated in the `test-results/` and `playwright-report/` directories.

The summarized, version-controlled benchmark data is automatically generated in `BENCHMARK_RESULTS.md` when you run one of the benchmark commands.
